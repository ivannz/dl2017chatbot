{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train the chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is remotely based on the amalgamation of [this](https://github.com/saltypaul/Seq2Seq-Chatbot)\n",
    "and [that](https://github.com/marekrei/sequence-labeler) repositories. The images are taken from\n",
    "the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll definitely need to plot something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Uninterruptible section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "class DelayedKeyboardInterrupt(object):\n",
    "    def __enter__(self):\n",
    "        self.signal_received = False\n",
    "        self.old_handler = signal.getsignal(signal.SIGINT)\n",
    "        signal.signal(signal.SIGINT, self.handler)\n",
    "\n",
    "    def handler(self, sig, frame):\n",
    "        self.signal_received = (sig, frame)\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.signal(signal.SIGINT, self.old_handler)\n",
    "        if self.signal_received:\n",
    "            self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fix the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(0x0BADC0DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import Theano and Lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5103 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 980 Ti (0000:06:00.0)\n"
     ]
    }
   ],
   "source": [
    "# %env THEANO_FLAGS='device=cuda0,force_device=True,mode=FAST_RUN,floatX=float32'\n",
    "\n",
    "import theano\n",
    "theano.config.exception_verbosity = 'high'\n",
    "\n",
    "import theano.tensor as tt\n",
    "\n",
    "import lasagne\n",
    "from lasagne.utils import floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fix Lasagne's random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lasagne.random.set_rng(np.random.RandomState(0xDEADC0DE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the line lookup table. It was generated from the [Cornell Movie--Dialogs Corpus](https://people.mpi-sws.org/~cristian/Cornell_Movie-Dialogs_Corpus.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"../processed_lines.json\", \"r\", encoding=\"utf-8\") as fin:\n",
    "    db_lines = {k: \"\\x02\" + v + \"\\x03\" for k, v in json.load(fin).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the diaogues into Q&A pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"../processed_dialogues.json\", \"r\", encoding=\"utf-8\") as fin:\n",
    "    db_dialogues = json.load(fin)\n",
    "\n",
    "qa_pairs = []\n",
    "for lines in db_dialogues:\n",
    "    qa_pairs.extend(zip(lines[:-1], lines[1:]))\n",
    "del db_dialogues\n",
    "\n",
    "# for easier indexing\n",
    "qa_pairs = np.array(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_counts = Counter(c for l in db_lines.values() for c in l[1:-1])\n",
    "\n",
    "## It is very important that these service characters be added first\n",
    "vocab = [\"\\x02\", \"\\x03\"]\n",
    "vocab += [c for c, f in token_counts.items()]\n",
    "\n",
    "token_to_index = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A function to lines into character id vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def as_matrix(lines, max_len=None):\n",
    "    if isinstance(lines, str):\n",
    "        lines = [lines]\n",
    "\n",
    "    length = max(map(len, lines))\n",
    "    length = min(length, max_len or length)\n",
    "\n",
    "    matrix = np.full((len(lines), length), -1, dtype='int32')\n",
    "    for i, line in enumerate(lines):\n",
    "        row_ix = [token_to_index.get(c, -1)\n",
    "                  for c in line[:length]]\n",
    "\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A function to convert History-Reply pairs into character id matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def retrieve_sentences(pairs):\n",
    "    enc, dec = [], []\n",
    "    for q, a in pairs:\n",
    "        enc.append(db_lines[q])\n",
    "        dec.append(db_lines[a])\n",
    "    return enc, dec\n",
    "\n",
    "def get_matrices(pairs, max_len=None):\n",
    "    enc, dec = retrieve_sentences(pairs)\n",
    "    return as_matrix(enc, max_len), as_matrix(dec, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A function to sample a batch from History-Reply pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size=32, max_len=None):\n",
    "    n_batches = (len(qa_pairs) + batch_size - 1) // batch_size\n",
    "    indices_ = random_state.permutation(len(qa_pairs))\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        yield get_matrices(qa_pairs[indices_[i::n_batches]], max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define a simple seq2seq network (preferably in Lasagne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, EmbeddingLayer\n",
    "from lasagne.layers import GRULayer, DenseLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "\n",
    "from broadcast import BroadcastLayer, UnbroadcastLayer\n",
    "from lasagne.layers import SliceLayer\n",
    "\n",
    "from lasagne.layers.base import Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The architecture hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_file = \"pickles/simple_mdl_epoch-15.pkl\"\n",
    "if os.path.exists(model_file):\n",
    "    with open(model_file, \"rb\") as fin:\n",
    "        ver, *rest = pickle.load(fin)\n",
    "        assert (ver == \"2.0\") or (ver == \"GRULayer\")\n",
    "\n",
    "    hyper, vocab, weights = rest\n",
    "\n",
    "else:\n",
    "    hyper = {\n",
    "        \"n_embed_char\": 32,\n",
    "        \"n_hidden_decoder\": 256,\n",
    "        \"n_hidden_encoder\": 512,\n",
    "        \"n_recurrent_layers\": 2,\n",
    "        \"b_xfeed\": False,\n",
    "        \"b_project\": True\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_embed_char = hyper[\"n_embed_char\"]              # 32\n",
    "n_hidden_encoder = hyper[\"n_hidden_encoder\"]      # 256\n",
    "n_hidden_decoder = hyper[\"n_hidden_decoder\"]      # 512\n",
    "n_recurrent_layers = hyper[\"n_recurrent_layers\"]  # 2\n",
    "b_xfeed = hyper[\"b_xfeed\"]                        # False\n",
    "b_project = hyper[\"b_project\"]                    # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/saltypaul/Seq2Seq-Chatbot/master/pics/Training%20Phase.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Embedding subgraph (pinkish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A helper to create stacked RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers.base import Layer\n",
    "\n",
    "def gru_column(input, num_units, hidden, **kwargs):\n",
    "    kwargs.pop(\"only_return_final\", None)\n",
    "    assert isinstance(hidden, (list, tuple))\n",
    "\n",
    "    name = kwargs.pop(\"name\", \"default\")\n",
    "    column = [input]\n",
    "    for i, l_hidden in enumerate(hidden):\n",
    "        kwargs_ = kwargs.copy()\n",
    "        if isinstance(l_hidden, Layer):\n",
    "            kwargs_.pop(\"learn_init\", None)\n",
    "            kwargs_[\"hid_init\"] = l_hidden\n",
    "\n",
    "        layer = GRULayer(column[-1], num_units,\n",
    "                         name=os.path.join(name, \"gru_%02d\" % i),\n",
    "                         **kwargs_)\n",
    "        column.append(layer)\n",
    "    return column[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create readouts of the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gru_hidden_readout(column, indices):\n",
    "    hidden = []\n",
    "    for layer in column:\n",
    "        name = os.path.join(layer.name, \"slice\")\n",
    "        slice_ = SliceLayer(layer, indices, axis=1, name=name)\n",
    "        hidden.append(slice_)\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Tap into the common embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l_encoder_mask = InputLayer((None, None), name=\"encoder/mask\")\n",
    "l_encoder_embed = InputLayer((None, None, n_embed_char), name=\"encoder/input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sentence representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Construct layered GRU columns atop the embedding (we can also make parallel fwd / rev layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hidden = n_recurrent_layers * [None]\n",
    "enc_rnn_layers = gru_column(l_encoder_embed, n_hidden_encoder, hidden,\n",
    "                            mask_input=l_encoder_mask, learn_init=True,\n",
    "                            backwards=False, name=\"encoder\")\n",
    "\n",
    "enc_rnn_layers_sliced = gru_hidden_readout(enc_rnn_layers, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Tap into the common embedding layer but with decoder's own input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l_decoder_mask = InputLayer((None, None), name=\"decoder/mask\")\n",
    "l_decoder_embed = InputLayer((None, None, n_embed_char), name=\"decoder/input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Cross-feed is not currently supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert not b_xfeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Project the hidden state of the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if b_project or (n_hidden_encoder != n_hidden_decoder):\n",
    "    dec_hid_inputs = []\n",
    "    for layer in enc_rnn_layers_sliced:\n",
    "        l_project = DenseLayer(layer, n_hidden_decoder, nonlinearity=None,\n",
    "                               name=os.path.join(layer.name, \"proj\"))\n",
    "        dec_hid_inputs.append(l_project)\n",
    "else:\n",
    "    dec_hid_inputs = enc_layers_sliced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Construct layers of GRU-s which recieve the final state of the encoder's network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dec_rnn_layers = gru_column(l_decoder_embed, n_hidden_decoder, dec_hid_inputs,\n",
    "                            mask_input=l_decoder_mask, learn_init=True,\n",
    "                            backwards=False, name=\"decoder\")\n",
    "\n",
    "dec_rnn_layers_sliced = gru_hidden_readout(dec_rnn_layers, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Read the output of the top layer of the RNN and re-embed into the character space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l_decoder_reembedder = DenseLayer(dec_rnn_layers[-1], num_units=len(vocab),\n",
    "                                  nonlinearity=None, num_leading_axes=2,\n",
    "                                  name=\"decoder/project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Construct the softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l_bc = BroadcastLayer(l_decoder_reembedder, broadcasted_axes=(0, 1), name=\"decoder/bc\")\n",
    "l_softmax = NonlinearityLayer(l_bc, nonlinearity=lasagne.nonlinearities.softmax, name=\"decoder/softmax\")\n",
    "l_decoder_output = UnbroadcastLayer(l_softmax, l_bc, name=\"decoder/ub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Embedding layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The common embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l_input_char = InputLayer((None, None), name=\"char/input\")\n",
    "l_embed_char = EmbeddingLayer(l_input_char, len(vocab), n_embed_char, name=\"char/embed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lasagne.layers.set_all_param_values(l_embed_char,\n",
    "                                    weights[\"l_embed_char\"])\n",
    "lasagne.layers.set_all_param_values(l_decoder_reembedder,\n",
    "                                    weights[\"l_decoder_reembedder\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Collect the encoder input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "v_encoder_input = tt.imatrix(name=\"encoder/input\")\n",
    "v_encoder_embed = l_embed_char.get_output_for(v_encoder_input)\n",
    "\n",
    "inputs = {l_encoder_embed: v_encoder_embed,\n",
    "          l_encoder_mask: tt.ge(v_encoder_input, 0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And the decoder's inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "v_decoder_input = tt.imatrix(name=\"decoder/input\")\n",
    "v_decoder_embed = lasagne.layers.get_output(l_embed_char, v_decoder_input)\n",
    "\n",
    "inputs.update({l_decoder_embed: v_decoder_embed,\n",
    "               l_decoder_mask: tt.ge(v_decoder_input, 0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get the output of the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "v_decoder_output, v_decoder_mask = lasagne.layers.get_output(\n",
    "    [l_decoder_output, l_decoder_mask], inputs, deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Slice the output to match the forward character-level language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "v_predicted = v_decoder_output[:, :-1].reshape(\n",
    "    (-1, v_decoder_output.shape[-1]))\n",
    "\n",
    "v_targets = v_decoder_input[:, 1:].reshape((-1,))\n",
    "\n",
    "v_mask = v_decoder_mask[:, 1:].reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Construct the cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss_ij = lasagne.objectives.categorical_crossentropy(v_predicted, v_targets)\n",
    "loss = (loss_ij * v_mask).sum()\n",
    "loss /= v_mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It can be benefitial to project the character embeddings onto the unit sphere.\n",
    "However we are going to project the embeddings into the unit $l^2$ ball instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W_emb = l_embed_char.get_params()[0]\n",
    "\n",
    "op_project_embedding = theano.function([], updates={\n",
    "    W_emb: W_emb / tt.maximum(W_emb.norm(2, axis=-1, keepdims=True), 1.0)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "On the other hand we can always add $l^2$ regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    C_embed = 1e-1\n",
    "    loss += C_embed * W_emb.norm(2, axis=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Collect all trainable parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trainable = []\n",
    "trainable.extend(lasagne.layers.get_all_params(l_embed_char, trainable=True))\n",
    "trainable.extend(lasagne.layers.get_all_params(l_decoder_output, trainable=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get the updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "learning_rate = theano.shared(floatX(1e-3), name=\"eta\")\n",
    "\n",
    "# updates = lasagne.updates.sgd(loss, trainable, learning_rate)\n",
    "updates = lasagne.updates.adam(loss, trainable, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create the ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "op_train = theano.function([v_decoder_input, v_encoder_input], loss,\n",
    "                           updates=updates, givens={},\n",
    "                           mode=theano.Mode(optimizer=\"fast_run\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "op_test_loss = theano.function([v_decoder_input, v_encoder_input], loss,\n",
    "                               mode=theano.Mode(optimizer=\"fast_run\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "op_predict = theano.function([v_decoder_input, v_encoder_input],\n",
    "                             v_decoder_output,\n",
    "                             mode=theano.Mode(optimizer=\"fast_run\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# inputs_ = {l_encoder_embed: v_encoder_embed,\n",
    "#            l_encoder_mask: tt.ge(v_encoder_input, 0),\n",
    "#            l_decoder_embed: v_decoder_embed,\n",
    "#            l_decoder_mask: tt.ge(v_decoder_input, 0)}\n",
    "\n",
    "v_decoder_logits = lasagne.layers.get_output(l_decoder_reembedder, inputs, deterministic=True)\n",
    "\n",
    "op_predict_logits = theano.function([v_decoder_input, v_encoder_input],\n",
    "                                     v_decoder_logits, mode=theano.Mode(optimizer=\"fast_run\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[char/embed.W,\n",
       " encoder/gru_00.W_in_to_updategate,\n",
       " encoder/gru_00.W_hid_to_updategate,\n",
       " encoder/gru_00.b_updategate,\n",
       " encoder/gru_00.W_in_to_resetgate,\n",
       " encoder/gru_00.W_hid_to_resetgate,\n",
       " encoder/gru_00.b_resetgate,\n",
       " encoder/gru_00.W_in_to_hidden_update,\n",
       " encoder/gru_00.W_hid_to_hidden_update,\n",
       " encoder/gru_00.b_hidden_update,\n",
       " encoder/gru_00.hid_init,\n",
       " encoder/gru_00/slice/proj.W,\n",
       " encoder/gru_00/slice/proj.b,\n",
       " decoder/gru_00.W_in_to_updategate,\n",
       " decoder/gru_00.W_hid_to_updategate,\n",
       " decoder/gru_00.b_updategate,\n",
       " decoder/gru_00.W_in_to_resetgate,\n",
       " decoder/gru_00.W_hid_to_resetgate,\n",
       " decoder/gru_00.b_resetgate,\n",
       " decoder/gru_00.W_in_to_hidden_update,\n",
       " decoder/gru_00.W_hid_to_hidden_update,\n",
       " decoder/gru_00.b_hidden_update,\n",
       " encoder/gru_01.W_in_to_updategate,\n",
       " encoder/gru_01.W_hid_to_updategate,\n",
       " encoder/gru_01.b_updategate,\n",
       " encoder/gru_01.W_in_to_resetgate,\n",
       " encoder/gru_01.W_hid_to_resetgate,\n",
       " encoder/gru_01.b_resetgate,\n",
       " encoder/gru_01.W_in_to_hidden_update,\n",
       " encoder/gru_01.W_hid_to_hidden_update,\n",
       " encoder/gru_01.b_hidden_update,\n",
       " encoder/gru_01.hid_init,\n",
       " encoder/gru_01/slice/proj.W,\n",
       " encoder/gru_01/slice/proj.b,\n",
       " decoder/gru_01.W_in_to_updategate,\n",
       " decoder/gru_01.W_hid_to_updategate,\n",
       " decoder/gru_01.b_updategate,\n",
       " decoder/gru_01.W_in_to_resetgate,\n",
       " decoder/gru_01.W_hid_to_resetgate,\n",
       " decoder/gru_01.b_resetgate,\n",
       " decoder/gru_01.W_in_to_hidden_update,\n",
       " decoder/gru_01.W_hid_to_hidden_update,\n",
       " decoder/gru_01.b_hidden_update,\n",
       " decoder/project.W,\n",
       " decoder/project.b]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/saltypaul/Seq2Seq-Chatbot/master/pics/Eval.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A handy slicer (copied and modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def slice_(x, i, n):\n",
    "    s = x[..., slice(i, i + n)]\n",
    "    return s if n > 1 else tt.addbroadcast(s, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define one step of the scan function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generator's one step update function\n",
    "def generator_step_sm(x_tm1, h_tm1, m_tm1, tau, eps):\n",
    "    \"\"\"One step of the generative decoder version.\"\"\"\n",
    "    # x_tm1 is `BxT` one-hot, h_tm1 is `batch x ...`\n",
    "    # m_tm1 is `batch`, tau, eps are scalars\n",
    "\n",
    "    # collect the inputs\n",
    "    inputs = {l_decoder_embed: x_tm1.dimshuffle(0, \"x\", 1),\n",
    "              l_decoder_mask: m_tm1.dimshuffle(0, \"x\")}\n",
    "\n",
    "    # Connect the prev variables to the the hidden and stack state feeds\n",
    "    j = 0\n",
    "    for layer in dec_rnn_layers:\n",
    "        inputs[layer.hid_init] = slice_(h_tm1, j, layer.num_units)\n",
    "        j += layer.num_units\n",
    "\n",
    "    # Get the outputs\n",
    "    outputs = [l_decoder_reembedder] + dec_rnn_layers_sliced\n",
    "\n",
    "    # propagate through the decoder column\n",
    "    logit_t, *h_t_list = lasagne.layers.get_output(outputs, inputs,\n",
    "                                                   deterministic=True)\n",
    "\n",
    "    # Pack the hidden states\n",
    "    h_t = tt.concatenate(h_t_list, axis=-1)\n",
    "    \n",
    "    # Generate the next symbol: logit_t is `Bx1xV`\n",
    "    logit_t = logit_t[:, 0]\n",
    "    prob_t = tt.nnet.softmax(logit_t)\n",
    "\n",
    "    # Gumbel-softmax sampling: Gumbel (e^{-e^{-x}}) distributed random noise\n",
    "    gumbel = -tt.log(-tt.log(theano_random_state.uniform(size=logit_t.shape) + eps) + eps)\n",
    "#     logit_t = theano.ifelse.ifelse(tt.gt(tau, 0), gumbel + logit_t, logit_t)\n",
    "#     inv_temp = theano.ifelse.ifelse(tt.gt(tau, 0), 1.0 / tau, tt.constant(1.0))\n",
    "    logit_t = tt.switch(tt.gt(tau, 0), gumbel + logit_t, logit_t)\n",
    "    inv_temp = tt.switch(tt.gt(tau, 0), 1.0 / tau, tt.constant(1.0))\n",
    "\n",
    "    # Get the softmax: x_t is `BxV`\n",
    "    x_t = tt.nnet.softmax(logit_t * inv_temp)\n",
    "\n",
    "    # Get the best symbol\n",
    "    c_t = tt.cast(tt.argmax(x_t, axis=-1), \"int8\")\n",
    "\n",
    "    # Get the estimated probability of the picked symbol.\n",
    "    p_t = prob_t[tt.arange(c_t.shape[0]), c_t]\n",
    "\n",
    "    # Compute the mask and inhibit the propagation on a stop symbol.\n",
    "    # Recurrent layers return the previous state if m_tm1 is Fasle\n",
    "    m_t = m_tm1 & tt.gt(c_t, vocab.index(\"\\x03\"))\n",
    "    c_t = tt.switch(m_t, c_t, vocab.index(\"\\x03\"))\n",
    "\n",
    "    # There is no need to freeze the states as they will be frozen by\n",
    "    # the RNN passthrough according to the mask `m_t`.\n",
    "\n",
    "    # Embed the current character.\n",
    "    x_t = tt.dot(x_t, l_embed_char.W)\n",
    "\n",
    "    return x_t, h_t, m_t, p_t, c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create scalar inputs to the scan loop. Also initialize the random stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "theano_random_state = tt.shared_randomstreams.RandomStreams(seed=42)\n",
    "\n",
    "eps = tt.fscalar(\"generator/epsilon\")\n",
    "n_steps = tt.iscalar(\"generator/n_steps\")\n",
    "tau = tt.fscalar(\"generator/gumbel/tau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's compile an autofeeding generator with softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "v_gen_input = tt.imatrix(name=\"generator/Q_input\")\n",
    "\n",
    "v_gen_embed = lasagne.layers.get_output(l_embed_char, v_gen_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Helper functions to freeze the GRULayer's hidden input's initialization,\n",
    "if one is a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def GRULayer_freeze(layer, input):\n",
    "    assert isinstance(layer, GRULayer)\n",
    "    if isinstance(layer.hid_init, Layer):\n",
    "        return layer\n",
    "\n",
    "    assert not (layer.hid_init_incoming_index > 0)\n",
    "    assert isinstance(layer.hid_init, theano.compile.SharedVariable)\n",
    "\n",
    "    # Broadcast the fixed /learnt hidden init over the batch dimension\n",
    "    hid_init = tt.dot(tt.ones((input.shape[0], 1)), layer.hid_init)\n",
    "\n",
    "    # Create a fake Input Layer, which receives it as input\n",
    "    layer._old_hid_init = layer.hid_init\n",
    "    layer.hid_init = InputLayer((None, None), input_var=hid_init,\n",
    "                                name=os.path.join(layer.name,\n",
    "                                                  \"hid_init_fix\"))\n",
    "    \n",
    "    # Cache former values\n",
    "    layer._old_input_layers = layer.input_layers\n",
    "    layer._old_input_shapes = layer.input_shapes\n",
    "    layer._old_hid_init_incoming_index = layer.hid_init_incoming_index\n",
    "    \n",
    "    # Emulate hidden layer input (is in GRULayer/MergeLayer.__init__())\n",
    "    layer.input_layers.append(layer.hid_init)\n",
    "    layer.input_shapes.append(layer.hid_init.output_shape)\n",
    "    layer.hid_init_incoming_index = len(layer.input_layers) - 1\n",
    "\n",
    "    layer._layer_frozen = True\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Freeze the hidden inputs of the decoder layers, which do not tap into the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for layer in dec_rnn_layers:\n",
    "    GRULayer_freeze(layer, v_gen_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Readout the last state from the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputs = {l_encoder_embed: v_gen_embed,\n",
    "          l_encoder_mask: tt.ge(v_gen_input, 0)}\n",
    "outputs = [l.hid_init for l in dec_rnn_layers]\n",
    "\n",
    "dec_hid_inits = lasagne.layers.get_output(outputs, inputs,\n",
    "                                          deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Prepare the initial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "h_0 = tt.concatenate(dec_hid_inits, axis=-1)\n",
    "\n",
    "x_0 = tt.fill(tt.zeros((v_gen_input.shape[0],), dtype=\"int32\"),\n",
    "              vocab.index(\"\\x02\"))\n",
    "x_0 = lasagne.layers.get_output(l_embed_char, x_0)\n",
    "\n",
    "m_0 = tt.ones((v_gen_input.shape[0],), 'bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Add a scan op and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result, updates = theano.scan(generator_step_sm, sequences=None, n_steps=n_steps,\n",
    "                              outputs_info=[x_0, h_0, m_0, None, None],\n",
    "                              strict=False, return_list=True,\n",
    "                              non_sequences=[tau, eps], go_backwards=False,\n",
    "                              name=\"generator/scan\")\n",
    "x_t, h_t, m_t, p_t, c_t = [r.swapaxes(0, 1) for r in result]\n",
    "\n",
    "compile_mode = theano.Mode(optimizer=\"fast_run\", linker=\"cvm\")\n",
    "op_generate = theano.function([v_gen_input, n_steps, tau],\n",
    "                              [c_t, h_t, m_t, p_t],\n",
    "                              updates=updates, givens={eps: floatX(1e-20)},\n",
    "                              mode=compile_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This function undoes the frrezing by the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def GRULayer_unfreeze(layer):\n",
    "    assert isinstance(layer, GRULayer)\n",
    "    freeze_attr = [\"_layer_frozen\",\n",
    "                   \"_old_input_layers\", \"_old_input_shapes\",\n",
    "                   \"_old_hid_init_incoming_index\", \"_old_hid_init\"]\n",
    "    if not all(hasattr(layer, a) for a  in freeze_attr):\n",
    "        return layer\n",
    "\n",
    "    assert layer._layer_frozen\n",
    "    assert isinstance(layer.hid_init, Layer)\n",
    "    assert layer.hid_init.name.endswith(\"/hid_init_fix\")\n",
    "\n",
    "    # Thawe the frozen hidden input\n",
    "    layer.hid_init = layer._old_hid_init\n",
    "    layer.input_layers = layer._old_input_layers\n",
    "    layer.input_shapes = layer._old_input_shapes\n",
    "    layer.hid_init_incoming_index = layer._old_hid_init_incoming_index\n",
    "    \n",
    "    for attr in freeze_attr:\n",
    "        delattr(layer, attr)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Unfreeze the decoder's layer, so that those which do not tap in to the encoder,\n",
    "may continue to use / learn their own `hid_init` state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for layer in dec_rnn_layers:\n",
    "    GRULayer_unfreeze(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A generator procedure, which automatically select the best replies (lowest perplexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate(questions, n_steps, n_samples=10, tau=0, seed=None):\n",
    "    results = []\n",
    "    for question in questions:\n",
    "        # Replicate the query\n",
    "        question = np.repeat(question[np.newaxis], n_samples, axis=0)\n",
    "        if seed is not None:\n",
    "            theano_random_state.seed(seed)\n",
    "\n",
    "        x_t, h_t, m_t, p_t = op_generate(question, n_steps, tau)\n",
    "\n",
    "        # may produce NaN, but they are shifted in the back by arsort\n",
    "        perplexity, n_chars = (- np.log2(p_t) * m_t).sum(axis=-1), m_t.sum(axis=-1)\n",
    "        perplexity /= n_chars\n",
    "\n",
    "        result = []\n",
    "        for i in perplexity.argsort():\n",
    "            reply = \"\".join(map(vocab.__getitem__, x_t[i, :n_chars[i]]))\n",
    "            result.append((reply, perplexity[i]))\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample_qa():\n",
    "    sample = qa_pairs[random_state.choice(len(qa_pairs), 3)]\n",
    "    enc, dec = retrieve_sentences(sample)\n",
    "\n",
    "    replies = generate(as_matrix(enc), 140, tau=2**-5, n_samples=20)\n",
    "    for e, d, r in zip(enc, dec, replies):\n",
    "        tqdm.tqdm.write(\"|%-40.40s | %-30.30s | %-30.30s|\" % (e[1:-1], d[1:-1], r[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set the batch size and the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size, n_epochs = 160, 15\n",
    "epoch, loss_val_hist = 0, []\n",
    "\n",
    "model_path = os.path.join(\"pickles\", time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "filename_fmt_ = os.path.join(model_path, \"simple_mdl_epoch-%02d%s.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's train the shit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "progress_fmt_, interrupted = \"%(loss).3f\", False\n",
    "n_batches = (len(qa_pairs) + batch_size - 1) // batch_size\n",
    "while epoch < n_epochs:\n",
    "    try:\n",
    "        with tqdm.tqdm(total=n_batches) as progress_:\n",
    "            for i, (be, bd) in enumerate(generate_batch(batch_size, max_len=512)):\n",
    "                if (i % 100) == 0:\n",
    "                    sample_qa()\n",
    "\n",
    "                with DelayedKeyboardInterrupt():\n",
    "                    loss_val_hist.append(op_train(bd, be))\n",
    "                    # op_project_embedding()\n",
    "\n",
    "                progress_.postfix = progress_fmt_ % {\n",
    "                    \"loss\": np.mean(loss_val_hist[-100:]),\n",
    "                }\n",
    "                progress_.update(1)\n",
    "            # end for\n",
    "\n",
    "        # end with\n",
    "        epoch += 1\n",
    "    except KeyboardInterrupt:\n",
    "        interrupted = True\n",
    "\n",
    "    finally:\n",
    "        # retrieve the parameters\n",
    "        weights = {\n",
    "            \"l_embed_char\": lasagne.layers.get_all_param_values(l_embed_char),\n",
    "            \"l_decoder_reembedder\": lasagne.layers.get_all_param_values(l_decoder_reembedder)\n",
    "        }\n",
    "        filename = filename_fmt_ % (epoch, \"_interrupted\" if interrupted else \"\")\n",
    "        with open(filename, \"wb\") as fin:\n",
    "            pickle.dump((\"GRULayer\", hyper, vocab, weights), fin)\n",
    "\n",
    "        if interrupted:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(np.log(loss_val_hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate(as_matrix([\"\\x02\" + \"Hi, there.\" + \"\\x03\"]), 75, tau=1e-5, n_samples=200, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def softmax(x_ij, axis=-1):\n",
    "    x_ij = np.exp(x_ij - x_ij.max(axis=axis, keepdims=True))\n",
    "    return x_ij / x_ij.sum(axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "logits = op_predict_logits(be, bd)\n",
    "\n",
    "rasta = np.random.RandomState(42)\n",
    "gumbel = -np.log(-np.log(rasta.uniform(size=logits.shape)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "p = gumbel + logits\n",
    "test = p[0, -10]\n",
    "ttau = np.logspace(-20, 20, num=101, base=2)[:, np.newaxis]\n",
    "test = softmax(test[np.newaxis] / ttau)\n",
    "\n",
    "plt.plot(np.log2(ttau), test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.hist(gumbel.flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for k, v, in zip(lasagne.layers.get_all_params(l_decoder_reembedder),\n",
    "                 weights[\"l_decoder_reembedder\"]):\n",
    "    print (k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list(zip(trainable[1:], weights[\"l_decoder_reembedder\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for be, bd in generate_batch(32, max_len=512):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "op_train(bd, be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ass = h_0.eval({v_gen_input: be})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(ass[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The older generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generator's one step update function\n",
    "def generator_step(x_tm1, h_tm1, m_tm1, tau, eps):\n",
    "    \"\"\"One step of the generative decoder version.\"\"\"\n",
    "    # x_tm1 is `batch` int8, h_tm1 is `batch x ...`\n",
    "    # m_tm1 is `batch`, tau, eps are scalars\n",
    "\n",
    "    # embed the previous character. x_t is `batch x embed`\n",
    "    x_t = l_embed_char.get_output_for(x_tm1, deterministic=True)\n",
    "\n",
    "    # collect the inputs\n",
    "    inputs = {l_decoder_embed: x_t.dimshuffle(0, \"x\", 1),\n",
    "              l_decoder_mask: m_tm1.dimshuffle(0, \"x\")}\n",
    "\n",
    "    # Connect the prev variables to the the hidden and stack state feeds\n",
    "    j = 0\n",
    "    for layer in dec_rnn_layers:\n",
    "        inputs[layer.hid_init] = slice_(h_tm1, j, layer.num_units)\n",
    "        j += layer.num_units\n",
    "\n",
    "    # Get the outputs\n",
    "    outputs = [l_decoder_reembedder] + dec_rnn_layers_sliced\n",
    "\n",
    "    # propagate through the decoder column\n",
    "    logit_t, *h_t_list = lasagne.layers.get_output(outputs, inputs,\n",
    "                                                   deterministic=True)\n",
    "\n",
    "    logit_t = logit_t[:, 0]\n",
    "    prob_t = tt.nnet.softmax(logit_t)\n",
    "\n",
    "    # Gumbel-softmax sampling: Gumbel (e^{-e^{-x}}) distributed random noise\n",
    "    gumbel = -tt.log(-tt.log(theano_random_state.uniform(size=logit_t.shape) + eps) + eps)\n",
    "#     logit_t = theano.ifelse.ifelse(tt.gt(tau, 0), gumbel + logit_t, logit_t)\n",
    "#     inv_temp = theano.ifelse.ifelse(tt.gt(tau, 0), 1.0 / tau, tt.constant(1.0))\n",
    "    logit_t = tt.switch(tt.gt(tau, 0), gumbel + logit_t, logit_t)\n",
    "    inv_temp = tt.switch(tt.gt(tau, 0), 1.0 / tau, tt.constant(1.0))\n",
    "\n",
    "    # Pick one element\n",
    "    x_t = tt.cast(tt.argmax(tt.nnet.softmax(logit_t * inv_temp), axis=-1), x_tm1.dtype)\n",
    "\n",
    "    # Pack the hidden states\n",
    "    h_t = tt.concatenate(h_t_list, axis=-1)\n",
    "\n",
    "    # Compute the mask and inhibit the propagation on a stop symbol.\n",
    "    # Recurrent layers return the previous state if m_tm1 is Fasle\n",
    "    m_t = m_tm1 & tt.gt(x_t, vocab.index(\"\\x03\"))\n",
    "    x_t = tt.switch(m_t, x_t, vocab.index(\"\\x03\"))\n",
    "\n",
    "    # There is no need to freeze the states as they will be frozen by\n",
    "    # the RNN passthrough according to the mask `m_t`.\n",
    "\n",
    "    # Get the estimated probability of the picked symbol.\n",
    "    p_t = prob_t[tt.arange(x_t.shape[0]), x_t]\n",
    "    return x_t, h_t, m_t, p_t\n",
    "\n",
    "h_0 = tt.concatenate(dec_hid_inits, axis=-1)\n",
    "\n",
    "x_0 = tt.fill(tt.zeros((v_gen_input.shape[0],), dtype=\"int32\"),\n",
    "              vocab.index(\"\\x02\"))\n",
    "\n",
    "m_0 = tt.ones((v_gen_input.shape[0],), 'bool')\n",
    "\n",
    "result, updates = theano.scan(generator_step, sequences=None, n_steps=n_steps,\n",
    "                              outputs_info=[x_0, h_0, m_0, None],\n",
    "                              strict=False, return_list=True,\n",
    "                              non_sequences=[tau, eps], go_backwards=False,\n",
    "                              name=\"generator/scan\")\n",
    "x_t, h_t, m_t, p_t = [r.swapaxes(0, 1) for r in result]\n",
    "\n",
    "compile_mode = theano.Mode(optimizer=\"fast_run\", linker=\"cvm\")\n",
    "op_generate = theano.function([v_gen_input, n_steps, tau],\n",
    "                              [x_t, h_t, m_t, p_t],\n",
    "                              updates=updates, givens={eps: floatX(1e-20)},\n",
    "                              mode=compile_mode)\n",
    "\n",
    "def generate(questions, n_steps, n_samples=10, tau=0, seed=None):\n",
    "    results = []\n",
    "    for question in questions:\n",
    "        # Replicate the query\n",
    "        question = np.repeat(question[np.newaxis], n_samples, axis=0)\n",
    "        if seed is not None:\n",
    "            theano_random_state.seed(seed)\n",
    "        x_t, h_t, m_t, p_t = op_generate(question, n_steps, tau)\n",
    "\n",
    "        # may produce NaN, but they are shifted in the back by arsort\n",
    "        perplexity, n_chars = (- np.log2(p_t) * m_t).sum(axis=-1), m_t.sum(axis=-1)\n",
    "        perplexity /= n_chars\n",
    "\n",
    "        result = []\n",
    "        for i in perplexity.argsort():\n",
    "            reply = \"\".join(map(vocab.__getitem__, x_t[i, :n_chars[i]]))\n",
    "            result.append((reply, perplexity[i]))\n",
    "        results.append(result)\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
