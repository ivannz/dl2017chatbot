{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train the chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook replicates this [article](http://arxiv.org/abs/1503.01007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll definitely need to plot something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Uninterruptible section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "class DelayedKeyboardInterrupt(object):\n",
    "    def __enter__(self):\n",
    "        self.signal_received = False\n",
    "        self.old_handler = signal.getsignal(signal.SIGINT)\n",
    "        signal.signal(signal.SIGINT, self.handler)\n",
    "\n",
    "    def handler(self, sig, frame):\n",
    "        self.signal_received = (sig, frame)\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.signal(signal.SIGINT, self.old_handler)\n",
    "        if self.signal_received:\n",
    "            self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fix the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(0x0BADC0DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import Theano and Lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5103 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 980 Ti (0000:06:00.0)\n"
     ]
    }
   ],
   "source": [
    "# %env THEANO_FLAGS='device=cuda0,force_device=True,mode=FAST_RUN,floatX=float32'\n",
    "\n",
    "import theano\n",
    "theano.config.exception_verbosity = 'high'\n",
    "\n",
    "import theano.tensor as tt\n",
    "\n",
    "import lasagne\n",
    "from lasagne.utils import floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fix Lasagne's random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lasagne.random.set_rng(np.random.RandomState(0xDEADC0DE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## It is very important that these service characters be added first\n",
    "vocab = [\">\", \"<\", \"a\", \"b\", \"c\"]\n",
    "token_to_index = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A function to lines into character id vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def as_matrix(lines, max_len=None):\n",
    "    if isinstance(lines, str):\n",
    "        lines = [lines]\n",
    "\n",
    "    max_len = max_len or max(map(len, lines))\n",
    "    matrix = np.full((len(lines), max_len), -1, dtype='int32')\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        row_ix = [token_to_index.get(c, -1)\n",
    "                  for c in line[:max_len]]\n",
    "\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A function to sample a batch from History-Reply pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size=32, max_seq_len=None):\n",
    "    sequences = []\n",
    "    for i in range(batch_size):\n",
    "        n, m = random_state.randint(1, 10, size=(2,))\n",
    "        sequences.append(\">\" + \"a\" * n + \"b\" * m + \"c\" * (n + m) + \"<\")\n",
    "    return as_matrix(sequences, max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define a simple seq2seq network (preferably in Lasagne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, EmbeddingLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.layers import SliceLayer\n",
    "\n",
    "from GRUwithStack import GRUStackLayer, GRUStackReadoutLayer\n",
    "from broadcast import BroadcastLayer, UnbroadcastLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The architecture hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_embed_char, n_hidden_decoder = len(vocab), 32\n",
    "n_stack_depth, n_stack_width = 50, 3\n",
    "n_recurrent_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Embedding subgraph (pinkish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The common embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l_input_char = InputLayer((None, None), name=\"char/input\")\n",
    "l_embed_char = EmbeddingLayer(l_input_char, len(vocab), n_embed_char, name=\"char/embed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Encoder-decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Tap into the common embedding layer but with decoder's own input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "v_input_decoder = tt.imatrix(name=\"decoder/input\")\n",
    "v_embed_decoder = lasagne.layers.get_output(l_embed_char, v_input_decoder)\n",
    "\n",
    "l_decoder_mask = InputLayer((None, None), name=\"decoder/mask\", input_var=tt.ge(v_input_decoder, 0))\n",
    "l_decoder_embed = InputLayer((None, None, n_embed_char), name=\"decoder/input\", input_var=v_embed_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Construct layers of GRU-s which recieve the final state of the encoder's network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dec_layers = [l_decoder_embed]\n",
    "dec_layers_stack = [None]\n",
    "for layer_num in range(n_recurrent_layers):\n",
    "    name = os.path.join(\"decoder\", \"GRU_%02d\" % layer_num)\n",
    "\n",
    "    v_stack_input = tt.zeros((v_input_decoder.shape[0], n_stack_depth, n_stack_width),\n",
    "                             dtype=theano.config.floatX)\n",
    "\n",
    "    l_stack_input = InputLayer((None, n_stack_depth, n_stack_width),\n",
    "                               input_var=v_stack_input,\n",
    "                               name=os.path.join(name, \"stack\"))\n",
    "\n",
    "    v_hid_input = tt.zeros((v_input_decoder.shape[0], n_hidden_decoder),\n",
    "                           dtype=theano.config.floatX)\n",
    "\n",
    "    l_hid_input = InputLayer((None, n_hidden_decoder),\n",
    "                             input_var=v_hid_input,\n",
    "                             name=os.path.join(name, \"hidden\"))\n",
    "\n",
    "    gru_layer = GRUStackLayer(dec_layers[-1], l_stack_input, n_hidden_decoder,\n",
    "                              hid_init=l_hid_input, mask_input=l_decoder_mask,\n",
    "                              backwards=False, learn_init=False, name=name)\n",
    "\n",
    "    dec_layers.append(gru_layer)\n",
    "    dec_layers_stack.append(\n",
    "        SliceLayer(GRUStackReadoutLayer(gru_layer,\n",
    "                                        name=os.path.join(name, \"stack_readout\")),\n",
    "                   -1, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"http://s32.postimg.org/vefrk7vqt/stack_gru.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Read the output of the top layer of the RNN and re-embed into the character space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l_decoder_reembedder = DenseLayer(dec_layers[-1], num_units=len(vocab),\n",
    "                                  nonlinearity=None, num_leading_axes=2,\n",
    "                                  name=\"decoder/project\")\n",
    "\n",
    "l_bc = BroadcastLayer(l_decoder_reembedder, broadcasted_axes=(0, 1), name=\"decoder/bc\")\n",
    "l_softmax = NonlinearityLayer(l_bc, nonlinearity=lasagne.nonlinearities.softmax, name=\"decoder/softmax\")\n",
    "l_decoder_output = UnbroadcastLayer(l_softmax, l_bc, name=\"decoder/ub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get the output of the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "v_decoder_output, v_decoder_mask = lasagne.layers.get_output(\n",
    "    [l_decoder_output, l_decoder_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Slice the output to match the forward character-level language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predicted = v_decoder_output[:, :-1].reshape((-1, v_decoder_output.shape[-1]))\n",
    "targets = v_input_decoder[:, 1:].reshape((-1,))\n",
    "mask = v_decoder_mask[:, 1:].reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Construct the cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = (lasagne.objectives.categorical_crossentropy(predicted, targets) * mask).sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Add $l^2$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from lasagne.regularization import regularize_network_params, l2\n",
    "\n",
    "# reg_l2 = regularize_network_params(l_decoder_output, l2) * 10e-5\n",
    "\n",
    "# loss += reg_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Collect all trainable parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trainable = []\n",
    "trainable.extend(lasagne.layers.get_all_params(l_embed_char, trainable=True))\n",
    "trainable.extend(lasagne.layers.get_all_params(l_decoder_output, trainable=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get the updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "learning_rate = theano.shared(floatX(1e-3), name=\"eta\")\n",
    "updates = lasagne.updates.adam(loss, trainable, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create the ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inazarov/anaconda3/envs/py36/lib/python3.6/site-packages/theano/tensor/basic.py:5130: UserWarning: flatten outdim parameter is deprecated, use ndim instead.\n",
      "  \"flatten outdim parameter is deprecated, use ndim instead.\")\n"
     ]
    }
   ],
   "source": [
    "op_train = theano.function([v_input_decoder], loss,\n",
    "                           updates=updates, givens={},\n",
    "                           mode=theano.Mode(optimizer=\"fast_run\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "op_test_loss = theano.function([v_input_decoder], loss,\n",
    "                               mode=theano.Mode(optimizer=\"fast_run\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "op_predict = theano.function([v_input_decoder], predicted,\n",
    "                               mode=theano.Mode(optimizer=\"fast_run\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define one step of the scan function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gen_step(x_tm1, h_tm1, m_tm1, s_tm1, tau, eps):\n",
    "    \"\"\"One step of the generative decoder version.\"\"\"\n",
    "    # x_tm1 is `batch` int8, h_tm1 is `batch x ...`\n",
    "    # m_tm1 is `batch`, tau, eps are scalars\n",
    "\n",
    "    # A handy slicer (copied and modified)\n",
    "    def slice_(x, i, n):\n",
    "        s = x[..., slice(i, i + n)]\n",
    "        return s if n > 1 else tt.addbroadcast(s, -1)\n",
    "\n",
    "    # embed the previous character. x_t is `batch x embed`\n",
    "    x_t = l_embed_char.get_output_for(x_tm1, deterministic=True)\n",
    "\n",
    "    # collect the inputs and hidden init feeds\n",
    "    j = 0\n",
    "    inputs = {l_decoder_embed: x_t.dimshuffle(0, \"x\", 1),\n",
    "              l_decoder_mask: m_tm1.dimshuffle(0, \"x\")}\n",
    "    for layer in dec_layers[1:]:\n",
    "        inputs[layer.hid_init] = slice_(h_tm1, j, layer.num_units)\n",
    "        j += layer.num_units\n",
    "\n",
    "    j = 0\n",
    "    for layer in dec_layers[1:]:  # enc_layers_stack[1:]\n",
    "        layer = layer.input_layers[1]\n",
    "        dep, wid = layer.output_shape[-2:]\n",
    "        stack_slice_ = slice_(s_tm1, j, dep * wid)\n",
    "        inputs[layer] = stack_slice_.reshape((-1, dep, wid))\n",
    "        j += dep * wid\n",
    "\n",
    "    outputs = [l_decoder_reembedder]\n",
    "    for h, s in zip(dec_layers[1:], dec_layers_stack[1:]):\n",
    "        outputs.append(h)\n",
    "        outputs.append(s)\n",
    "\n",
    "    # propagate through the decoder column\n",
    "    logit_t, *rest = lasagne.layers.get_output(outputs, inputs, deterministic=True)\n",
    "    h_t_list, s_t_list = rest[::2], rest[1::2]\n",
    "\n",
    "    logit_t = logit_t[:, 0]\n",
    "    prob_t = tt.nnet.softmax(logit_t)\n",
    "\n",
    "    # Gumbel-softmax sampling\n",
    "    do_not_sample = tt.le(tau, 0)\n",
    "    gumbel = theano.ifelse.ifelse(\n",
    "        do_not_sample, tt.zeros_like(logit_t),\n",
    "        -tt.log(-tt.log(theano_random_state.uniform(size=logit_t.shape) + eps) + eps))\n",
    "\n",
    "    # Add Gumbel (e^{-e^{-x}}) distributed random noise\n",
    "    logit_t = (gumbel + logit_t) / theano.ifelse.ifelse(do_not_sample, 1.0, tau)\n",
    "\n",
    "    # Pick one element\n",
    "    x_t = tt.argmax(\n",
    "        theano.ifelse.ifelse(do_not_sample, prob_t,\n",
    "                             tt.nnet.softmax(logit_t)),\n",
    "        axis=-1)\n",
    "    \n",
    "#     # Gumbel-softmax sampling: Gumbel (e^{-e^{-x}}) distributed random noise\n",
    "#     gumbel = -tt.log(-tt.log(theano_random_state.uniform(size=logit_t.shape) + eps) + eps)\n",
    "#     logit_t = tt.switch(tt.gt(tau, 0), (gumbel + logit_t) / tau, logit_t)\n",
    "\n",
    "#     # Pick one element\n",
    "#     x_t = tt.argmax(tt.switch(tt.gt(tau, 0),\n",
    "#                               tt.nnet.softmax(logit_t),\n",
    "#                               prob_t), axis=-1)\n",
    "\n",
    "    # stop generation if a stop symbol was picked.\n",
    "    m_t = m_tm1 & tt.gt(x_t, vocab.index(\"<\"))\n",
    "\n",
    "    # Concatenate the hidden states, freezing them if necessary\n",
    "    h_t = tt.concatenate([v[:, 0] for v in h_t_list], axis=-1)\n",
    "    h_t = tt.switch(m_t.dimshuffle(0, 'x'), h_t, h_tm1)\n",
    "\n",
    "    # flatten and concatenate the stack state\n",
    "    s_t = tt.concatenate([v.flatten(ndim=2) for v in s_t_list], axis=-1)\n",
    "    s_t = tt.switch(m_t.dimshuffle(0, 'x'), s_t, s_tm1)\n",
    "\n",
    "    # Propagate the stop token\n",
    "    x_t = tt.cast(tt.switch(m_t, x_t, vocab.index(\"<\")), \"int8\")\n",
    "    p_t = prob_t[tt.arange(x_t.shape[0]), x_t]\n",
    "\n",
    "    return x_t, h_t, m_t, s_t, p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# x_t, h_t, m_t, p_t = op_generate(1, 120, 1e-4)\n",
    "\n",
    "# x_t[0]\n",
    "\n",
    "# m_t[0]\n",
    "\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create scalar inputs to the scan loop. Also initialize the random stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "theano_random_state = tt.shared_randomstreams.RandomStreams(seed=42)\n",
    "\n",
    "eps = tt.fscalar(\"generator/epsilon\")\n",
    "n_steps = tt.iscalar(\"generator/n_steps\")\n",
    "tau = tt.fscalar(\"generator/gumbel/tau\")\n",
    "\n",
    "n_batch = tt.iscalar(\"generator/n_batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Readout the last state of the stack (per dialogue in the batch) from the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "enc_stack_inits = [tt.zeros((n_batch, n_stack_depth, n_stack_width)) for l in dec_layers[1:]]\n",
    "enc_hid_inits = [tt.zeros((n_batch, n_hidden_decoder)) for l in dec_layers[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Prepare the initial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_0 = tt.fill(tt.zeros((n_batch, ), dtype=\"int32\"), vocab.index(\">\"))\n",
    "h_0 = tt.concatenate(enc_hid_inits, axis=-1)\n",
    "s_0 = tt.concatenate([v.flatten(ndim=2) for v in enc_stack_inits], axis=-1)\n",
    "m_0 = tt.ones_like(x_0, 'bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Add a scan op and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result, updates = theano.scan(gen_step, sequences=None, n_steps=n_steps,\n",
    "                              outputs_info=[x_0, h_0, m_0, s_0, None], return_list=True,\n",
    "                              non_sequences=[tau, eps], strict=False,\n",
    "                              go_backwards=False, name=\"generator/scan\")\n",
    "x_t, h_t, m_t, s_t, p_t = [r.swapaxes(0, 1) for r in result]\n",
    "\n",
    "compile_mode = theano.Mode(optimizer=\"fast_run\", linker=\"cvm\")\n",
    "op_generate = theano.function([n_batch, n_steps, tau], [x_t, h_t, m_t, p_t],\n",
    "                              updates=updates, givens={eps: floatX(1e-20)},\n",
    "                              mode=compile_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A generator procedure, which automatically select the best replies (shortest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate(n_batches, n_steps, tau=0):\n",
    "    x_t, h_t, m_t, p_t = op_generate(n_batches, n_steps, tau)\n",
    "\n",
    "    perplexity = (- np.log2(p_t) * m_t).sum(axis=-1)\n",
    "    perplexity /= m_t.sum(axis=-1)\n",
    "    order = perplexity.argsort()\n",
    "    x_t, perplexity = x_t[order], perplexity[order]\n",
    "\n",
    "    result = []\n",
    "    for reply in x_t:\n",
    "        reply_ = \"\".join([vocab[i] for i in reply])\n",
    "        result.append(reply_.replace(\"<\", \"\"))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample_qa():\n",
    "    replies = generate(10, 140, tau=1e-20)\n",
    "    tqdm.tqdm.write(\", \".join(replies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Reset the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epoch, loss_val_hist = 0, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's train the shit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/5000 [00:00<35:21,  2.36it/s, 1.612]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acbccc, ccbc, cc, cac, c, bcab, a, , , \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 51/5000 [00:21<34:45,  2.37it/s, 1.378]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbc, c, c, c, b, , , , , \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 101/5000 [00:42<34:16,  2.38it/s, 1.197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbbcbbcc, a, ab, abaaa, cb, , , , , \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 151/5000 [01:04<33:56,  2.38it/s, 0.884]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acaaaaabbbcbcccccccc, aa, aaaacbbbcbc, bacabbc, bbabc, b, cba, cccc, , \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 201/5000 [01:25<33:30,  2.39it/s, 0.631]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa, aaaaaaabbbcbbcccccccccccccccccc, aaabaabbbbcccccccccccccccccccc, aabaabbbbbbccccccccccccccccccc, aaaaaabbbccbbcccccccccccccccc, ababbbbcccccccccccccccccccc, bbbbbcbccccc, abaabaabccbccccc, b, b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 251/5000 [01:46<33:19,  2.37it/s, 0.470]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaabbbbcccccccccccc, aaaaaaabbbbbbccccccc, aaaaaaaabbbbccccc, aaaaabbbbbbbbbbcccccccccc, aabbbbbbcccccccccccc, aaaaaabbbbbbbbbbccccc, abbbbbbbbccccccc, aaaaababbbcccccccccc, bbbbbbbbccccc, baaaaaaacbbbbbbcccccccccccccccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 301/5000 [02:07<32:53,  2.38it/s, 0.404]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaabbbbbbccccccccccccccc, aaaaaaaabbbbbcccccccccccccccc, aaaaabbbbbcccccccc, aaaaabbbbcccccccccc, aaaaaabbbcccccccccc, aaaaaaabbcccccccccccccc, aaaabbbbcccccccccccccc, aaabbbbccccccccc, baaabbccccccccccccccc, acaabbbcccccccccccccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 351/5000 [02:28<32:39,  2.37it/s, 0.374]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaabbbbbbbbbccccccccccccc, aaaaaaabbbcccccccccccc, aaabbbbbcccccccccccc, aaaabbbbbbbcccc, aabbbbbbbccccccc, aaaaaaaabccccccccccc, aaaaaaaaaaaabccccccccccc, aaaaaaacbbcccccccccccc, aaaaabbccccc, ccbac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 401/5000 [02:49<32:11,  2.38it/s, 0.353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa, aaaaaabbbbbbbbcccccccccccc, aaaaaaaaabbbbcccccccccc, aaaabbbbbbccccccccc, aaaaaabbbbbcccccc, aabbbbbbbbcccccccccc, aaaaaaaabbcccccccccccc, aaabbbbcccccccc, aaaaaaacbbbbbbbbccccccccccccc, aaaaaaaaaaaabccccccccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 451/5000 [03:10<31:56,  2.37it/s, 0.324]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaabbbbbccccccccccc, aaaaaaabbccccccccc, aaaaaaabbccccccccc, aaaabbbbbbbbcccccccccccccc, aaabbbbbbcccccccccc, aaaaaaaabbbcbccccccccccccc, aaababbbbcbbbbbccccccccccccc, aaaabaccccccc, bbbccccc, aabcbccccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 501/5000 [03:31<31:24,  2.39it/s, 0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaabbbbbbbbbcccccccccccccc, aaaabbbbbbbbcccccccccccc, aaaaaaabbbcccccccccc, aaaabbbbbccccccccc, aaaaaaabbbccccc, aaaaaaabccccccc, aaaaabbcccccc, abbbbbbbbccccccccc, aabbbbbcccccccc, abbbbbcccccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 551/5000 [03:53<31:11,  2.38it/s, 0.268]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaaabbbbbbbccccccccccccccc, aaaaaaabbbbbccccccccccccc, aaaaaaaabbbccccccccccc, aaaaaaabbbbccccccccc, aaaaabbbbccccccccc, aaaaaaabbccccccc, aabbbbbbcccccccc, aabbbbbcccccccc, aabbbbcccccc, aaaaaaaaabbbbbccccccbcccccccccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 601/5000 [04:14<30:48,  2.38it/s, 0.255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaabbbbbbbbcccccccccccccc, aaaaabbbbbbbbcccccccccccccc, aaaaaabbbbcccccccccc, aaaaabbbcccccccc, abbbbbbbbbccccccccc, aaaaaaabcccccccc, aaaaaaaaaaabbbbbccccccccccccccccc, aaaaabbccccccc, aabbcccc, baaaacbbbcccccccccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 651/5000 [04:35<30:30,  2.38it/s, 0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaabbbbbbbbbcccccccccccccccc, aaaaaaaabbbbbbbbbccccccccccccccc, aaaaaaabbbbbcccccccccccc, aaaaabbbbbbbcccccccccccc, aaaaaabbbbbccccccccccc, aaabbbbbbbcccccccccc, aaaaaaaabccccccccc, abbbbbbbcccccccc, aaaaaaaaccbccccccccc, aabbccccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 701/5000 [04:56<29:58,  2.39it/s, 0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaabbbbbcccccccccccc, aaaaabbbbbbbcccccccccccc, aaaaaaaaaabbbbbccccccccccccccc, aaaabbbbbbcccccccccc, aabbbbbbcccccccc, abbbbbbbcccccccc, abbbbbbbcccccccc, aabbbbbccccccc, aaabbccccc, aaaabccaacccccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 751/5000 [05:17<29:46,  2.38it/s, 0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaabbbbbbbbbccccccccccccccc, aaaabbbbbbbccccccccccc, aaaaabbbbbcccccccccc, aabbbbbbbbcccccccccc, aaaaabbbbccccccccc, abbbbbbbbccccccccc, aaaaaabbbbbcccccccccccc, aaabbbcccccc, aabbbccccc, abbbbccccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 801/5000 [05:38<29:27,  2.38it/s, 0.229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaaaabbbbbbccccccccccccccc, aaaaabbbbbbbcccccccccccc, aaaaaaaabbbbcccccccccccc, aaaaaaaabbbbccccccccccc, aaaaaaabbbbccccccccccc, aaaaaabbbbcccccccccc, aaaaaabbbbcccccccccc, aaabbbbbbbbbbccccccccccccc, aaaaaaaaccccccccc, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 851/5000 [05:59<29:05,  2.38it/s, 0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaabbbbbbbbccccccccccccccc, aaaaaaabbbbbbccccccccccccc, aaaaaaabbbbbbccccccccccccc, aaaaabbbbbbbcccccccccccc, aaabbbbbbbbccccccccccc, aaaaaabbbbcccccccccc, aaaaaaaaacccccccccc, aaaaaaaaaaacccccccccccc, aaaaaabbbbcbccccccccccc, aabbcc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 901/5000 [06:21<28:44,  2.38it/s, 0.225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaabbbbbbbbbcccccccccccccc, aaaaaaaabbcccccccccc, aaabbbbbbbbbbccccccccccccc, aaaaaaaaabcccccccccc, aabbbbbbbbbbcccccccccccc, abbbbbbbbccccccccc, aaaaaaaabbbbbcbccccccccccccc, aaaaaabcccccc, aaabbcccc, abbcccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 951/5000 [06:42<28:22,  2.38it/s, 0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaabbbbbbbccccccccccccc, aaabbbbbbbbccccccccccc, aaabbbbbbccccccccc, aaaaaaaaabbbbbbbccccccccccccccccc, abbbbbbbbbcccccccccc, aaaaabbccccccc, aabbbccccc, aabbbccccc, aabbcccc, abbcccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1001/5000 [07:03<27:59,  2.38it/s, 0.221]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaabbbbbbbcccccccccccccc, aaaaaaabbbbbcccccccccccc, aaaabbbbbbbbbccccccccccccc, aabbbbbbbbcccccccccc, aaaaaaabbccccccccc, aaaaaaabbccccccccc, aaaaaaaaaabbcccccccccccc, aaabbbbbcccccccc, aaaaaabccccccc, aaabbccccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 1051/5000 [07:24<27:40,  2.38it/s, 0.221]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aabbbbbbbbcccccccccc, aaaaaabbbbcccccccccc, aaabbbbbbccccccccc, aaaaaaaaaabbcccccccccccc, aaaaaaabcccccccc, abbbbbbbbbccccccccc, aaaabbcccccc, aaabbbcccccc, abbbcccc, abbbcccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1101/5000 [07:45<27:22,  2.37it/s, 0.227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaabbbbbbbcccccccccccccc, aaaaabbbbbbbbccccccccccccc, aaaaaaaabbbbcccccccccccc, aaaaaabbbbbccccccccccc, aaabbbbbbbbbcccccccccccc, aaaabbbbbbbbbbcccccccccccccc, abbbbbbbccccccc, aaaabbbccccccc, abbbbbcccccc, abbbcccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1151/5000 [08:07<27:05,  2.37it/s, 0.231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaabbbbbbbbcccccccccccccc, aaaaaaabbbbbbccccccccccccc, aaaaaaabbbbbbccccccccccccc, aaaaaaaabbbbcccccccccccc, aaaaaaaaaabbbbbbcccccccccccccccc, aaaaaabbbbbccccccccccc, aaaaabbbbbcccccccccc, aaaaaaabbccccccccc, aaaaaaaaabcccccccccc, aabbbccccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1201/5000 [08:28<26:33,  2.38it/s, 0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaaaabbbbbbccccccccccccccc, aaaaaaaabbbbccccccccccc, aaaaaabbbbbccccccccccc, aaaaaaaaabcccccccccc, abbbbbbbbccccccccc, aaaaaabbbccccccccc, aaaaaaabcccccccc, aaaaaaabcccccccc, aaabbbcccccc, abbbcccc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c09ff1632fa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mDelayedKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mloss_val_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         progress_.postfix = progress_fmt_ % {\n",
      "\u001b[0;32m<ipython-input-3-4286ca868cd7>\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mold_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal_received\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mold_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal_received\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size, n_epochs = 128, 5000\n",
    "progress_fmt_ = \"%(loss).3f\"\n",
    "with tqdm.tqdm(total=n_epochs-epoch) as progress_:\n",
    "    while epoch < n_epochs:\n",
    "        batch = generate_batch(batch_size, max_seq_len=512)\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            loss_val_hist.append(op_train(batch))\n",
    "\n",
    "        progress_.postfix = progress_fmt_ % {\n",
    "            \"loss\": np.mean(loss_val_hist[-100:]),\n",
    "        }\n",
    "        progress_.update(1)\n",
    "        if (epoch % 50) == 0:\n",
    "            sample_qa()\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaaaaaaabbbbbbbccccccccccccccc',\n",
       " 'aaaaaaabbbbbbbbccccccccccccccc',\n",
       " 'aaaaaaaaabbbbbcccccccccccccc',\n",
       " 'aaaaabbbbbbbcccccccccccc',\n",
       " 'aaaaaaabbbbbcccccccccccc',\n",
       " 'aaabbbbbbbbbcccccccccccc',\n",
       " 'aaaaaaaaabbccccccccccc',\n",
       " 'aabbbbbbbccccccccc',\n",
       " 'aabbbbbccccccc',\n",
       " 'abbbbccccc']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(10, 150, tau=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate(as_matrix([\"\\x02\" + \"First, have theano installed.\" + \"\\x03\"]), 75, tau=1e-5, n_samples=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "weights = {\n",
    "    \"l_embed_char\": lasagne.layers.get_all_param_values(l_embed_char),\n",
    "    \"l_decoder_reembedder\": lasagne.layers.get_all_param_values(l_decoder_reembedder)\n",
    "}\n",
    "\n",
    "filename = \"trained_MyFirstBotMK2_with-stack_model_%s.pkl\" % time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "with open(filename, \"wb\") as fin:\n",
    "    pickle.dump((\"1.0\", vocab, weights), fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list(zip(trainable[1:], weights[\"l_decoder_reembedder\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
